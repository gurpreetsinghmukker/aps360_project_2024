{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check which devices are available. We check for gpu and mps\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "platform = os.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ab39751b6f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import tarfile\n",
    "import transform_utilities\n",
    "import visualization_utilities\n",
    "import gtzan_dataset\n",
    "import models\n",
    "import mtg_contrastive\n",
    "import mtg_contrastive_mel\n",
    "import infonce_loss\n",
    "import barlow_twin_loss\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import dataloader\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "\n",
    "importlib.reload(transform_utilities)\n",
    "importlib.reload(visualization_utilities)\n",
    "importlib.reload(gtzan_dataset)\n",
    "importlib.reload(models)\n",
    "importlib.reload(mtg_contrastive)\n",
    "importlib.reload(mtg_contrastive_mel)\n",
    "importlib.reload(infonce_loss)\n",
    "importlib.reload(barlow_twin_loss)\n",
    "\n",
    "\n",
    "from transform_utilities import *\n",
    "from visualization_utilities import *\n",
    "from gtzan_dataset import *\n",
    "from models import *\n",
    "from mtg_contrastive import MTGContrastiveDataset\n",
    "from mtg_contrastive_mel import MTG_Mel_ContrastiveDataset, worker_init_fn\n",
    "from infonce_loss import InfoNCE\n",
    "from barlow_twin_loss import BarlowTwinsLoss\n",
    "\n",
    "\n",
    "p_test = 0.2\n",
    "p_train_val = 1 - p_test\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for the deep model trained end to end on GTZAN\n",
    "deep_base_output_dir = output_dir / 'base_gtzan'\n",
    "deep_base_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Define the path for the checkpoints for the pretrained contrastive model\n",
    "contrastive_output_dir = output_dir / 'contrastive'\n",
    "contrastive_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Define the path for the checkpoints for the pretrained barlow model\n",
    "barlow_contrastive_output_dir = output_dir / 'barlow_contrastive'\n",
    "barlow_contrastive_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Path for the mlp that accepts the embeddings (un-normalized) from the contrastive model\n",
    "contrastive_classifier_output_dir = output_dir / 'contrastive_classifier'\n",
    "contrastive_classifier_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Path for the mlp that accepts the logits from the contrastive model\n",
    "contrastive_classifier_embedder_only_output_dir = output_dir / 'contrastive_classifier_embedder_only'\n",
    "contrastive_classifier_embedder_only_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Path for the mlp that accepts the embeddings (normalized) from the barlow model\n",
    "contrastive_classifier_barlow_output_dir = output_dir / 'contrastive_classifier_barlow'\n",
    "contrastive_classifier_barlow_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Path for the mlp that accepts the logits from the barlow model\n",
    "contrastive_classifier_barlow_embedder_only_output_dir = output_dir / 'contrastive_classifier_barlow_embedder_only'\n",
    "contrastive_classifier_barlow_embedder_only_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Path for baseline random forest\n",
    "baseline_random_forest_output_dir = output_dir / 'baseline_random_forest'\n",
    "baseline_random_forest_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Path for the random forest that uses contrastive embeddings\n",
    "contrastive_random_forest_output_dir = output_dir / 'contrastive_random_forest'\n",
    "contrastive_random_forest_output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 168960\n"
     ]
    }
   ],
   "source": [
    "references_sample_rate = 22050\n",
    "mtg_mel_path = mtg_path = Path('') / 'mtg_mel'\n",
    "mtg_dataset = MTG_Mel_ContrastiveDataset(mtg_mel_path, references_sample_rate, mask_prob=0)\n",
    "print(f\"Dataset size: {len(mtg_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_base_gtzan_classifier(model, train_loader, val_loader, epochs, learning_rate, output_dir: Path):\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "\n",
    "    t_loss_history = []\n",
    "    v_loss_history = []\n",
    "    v_acc_history = []\n",
    "\n",
    "    step = 0\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        for (waveform, mel_spectrogram, label) in tqdm(train_loader):\n",
    "            mel_spectrogram = mel_spectrogram.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(mel_spectrogram)\n",
    "            loss = criterion(output, label)\n",
    "            t_loss_history.append((step, loss.item()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            step += 1\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            val_accuracies = []\n",
    "            for (waveform, mel_spectrogram, label) in val_loader:\n",
    "                mel_spectrogram = mel_spectrogram.to(device)\n",
    "                label = label.to(device)\n",
    "\n",
    "                output = model(mel_spectrogram)\n",
    "                val_loss = criterion(output, label)\n",
    "                val_losses.append(val_loss.item())\n",
    "                val_acc = (output.argmax(dim=1) == label).float().mean()\n",
    "                val_accuracies.append(val_acc.item())\n",
    "\n",
    "            val_loss = torch.tensor(val_losses).mean()\n",
    "            v_loss_history.append((step, val_loss.item()))\n",
    "            val_acc = torch.tensor(val_accuracies).mean()\n",
    "            v_acc_history.append((step, val_acc.item()))\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), output_dir / 'best.pth')\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f} | Val Acc: {val_acc.item():.4f}\")\n",
    "\n",
    "    return t_loss_history, v_loss_history, v_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(model_name, lr, batch_size):\n",
    "    return f\"{model_name}_LR({lr})_BS({batch_size})\"\n",
    "\n",
    "def train_contrastive_model(model, train_loader, val_loader, epochs, lr, batch_size, criterion, output_dir: Path, t_loss_history=None, v_loss_history=None, checkpoint_file=None, device = None):\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    criterion = criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "\n",
    "    t_loss_history = [] if t_loss_history is None else t_loss_history\n",
    "    v_loss_history = [] if v_loss_history is None else v_loss_history\n",
    "\n",
    "    step = 0\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    if checkpoint_file is not None:\n",
    "        print(f\"Recovering model from {checkpoint_file}\")\n",
    "        checkpoint = torch.load(checkpoint_file)\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
    "        step = checkpoint[\"step\"]\n",
    "        best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "        recovered_epoch = checkpoint[\"epoch\"]\n",
    "        recovered_t_loss_history = checkpoint[\"t_loss_history\"]\n",
    "        recovered_v_loss_history = checkpoint[\"v_loss_history\"]\n",
    "\n",
    "        print(f\"Recovered model from epoch {recovered_epoch} with best val loss {best_val_loss:.4f}\")\n",
    "\n",
    "        # Copy the recovered loss histories into the current ones\n",
    "        t_loss_history.extend(recovered_t_loss_history)\n",
    "        v_loss_history.extend(recovered_v_loss_history)\n",
    "    else:\n",
    "        recovered_epoch = 0\n",
    "\n",
    "    for epoch in range(recovered_epoch, epochs):\n",
    "        model.train()\n",
    "\n",
    "        train_losses = []\n",
    "        for (waveform1, waveform2), (mel_spectrogram1, mel_spectrogram2), label in tqdm(train_loader):\n",
    "            \n",
    "            loss_acc = 0\n",
    "            optimizer.zero_grad()\n",
    "            if batch_size > 64:\n",
    "                for iter in range(batch_size // 64):\n",
    "                    mini_batch_1 = mel_spectrogram1[iter*64:(iter+1)*64].to(device)\n",
    "                    mini_batch_2 = mel_spectrogram2[iter*64:(iter+1)*64].to(device)\n",
    "                    mini_batch = [mini_batch_1, mini_batch_2]\n",
    "                    output = model(mini_batch)\n",
    "                    loss = criterion(output)\n",
    "                    loss_acc += loss.item()\n",
    "                    loss.backward()\n",
    "            else:\n",
    "                mel_spectrogram1 = mel_spectrogram1.to(device)\n",
    "                mel_spectrogram2 = mel_spectrogram2.to(device)\n",
    "                mel_spectrogram = [mel_spectrogram1, mel_spectrogram2]\n",
    "                output = model(mel_spectrogram)\n",
    "                loss = criterion(output)\n",
    "                loss.backward()\n",
    "            # mel_spectrogram1 = mel_spectrogram1.to(device)\n",
    "            # mel_spectrogram2 = mel_spectrogram2.to(device)\n",
    "            # mel_spectrogram = [mel_spectrogram1, mel_spectrogram2]\n",
    "            # Stack the spectrograms along the batch dimension\n",
    "            # mel_spectrogram = torch.cat([mel_spectrogram1, mel_spectrogram2], dim=0)\n",
    "\n",
    "            # print(mel_spectrogram.shape)\n",
    "\n",
    "            # output = model(mel_spectrogram)\n",
    "\n",
    "            # Split the output into the original batches\n",
    "            # output1, output2 = torch.split(output, output.shape[0] // 2, dim=0)\n",
    "\n",
    "            # loss = criterion(output)\n",
    "            t_loss_history.append((step, loss_acc))\n",
    "            train_losses.append(loss_acc)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            step += 1\n",
    "        avg_train_loss = torch.tensor(train_losses).mean()\n",
    "\n",
    "        model.eval()\n",
    "        # print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss.item():.4f}\")\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            val_accuracies = []\n",
    "            for (waveform1, waveform2), (mel_spectrogram1, mel_spectrogram2), label in tqdm(val_loader):\n",
    "                mel_spectrogram1 = mel_spectrogram1.to(device)\n",
    "                mel_spectrogram2 = mel_spectrogram2.to(device)\n",
    "                mel_spectrogram = [mel_spectrogram1, mel_spectrogram2]\n",
    "                \n",
    "                output = model(mel_spectrogram)\n",
    "\n",
    "                # output1, output2 = torch.split(output, output.shape[0] // 2, dim=0)\n",
    "                val_loss = criterion(output)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            val_loss = torch.tensor(val_losses).mean()\n",
    "            v_loss_history.append((step, val_loss.item()))\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), output_dir / f\"{get_model_name(model.name, lr, batch_size)}_best.pth\")\n",
    "\n",
    "            # Save the latest model\n",
    "            save_data = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scheduler\": scheduler.state_dict(),\n",
    "                \"step\": step,\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"best_val_loss\": best_val_loss,\n",
    "                \"t_loss_history\": t_loss_history,\n",
    "                \"v_loss_history\": v_loss_history\n",
    "            }\n",
    "            torch.save(save_data, output_dir / f\"{get_model_name(model.name, lr, batch_size)}_latest_checkpoint.pth\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss.item():.4f} | Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "    return t_loss_history, v_loss_history\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_params(batch_size, learning_rate, epochs, criterion, model, output_dir):\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    batch_size = batch_size\n",
    "    learning_rate = learning_rate\n",
    "\n",
    "    # Using the MTG Dataset\n",
    "    # train_folders = [\n",
    "    #     \"00\", \"01\", \"02\", \"03\",\n",
    "    #     \"04\", \"05\", \"06\", \"07\",\n",
    "    #     \"08\", \"09\", \"10\", \"11\",\n",
    "    #     \"12\", \"13\", \"14\", \"15\",\n",
    "    #     \"16\", \"17\", \"18\", \"19\"\n",
    "    # ]\n",
    "    \n",
    "    train_folders = [\"00\"]\n",
    "    \n",
    "    val_folders = [\n",
    "        \"20\"\n",
    "    ]\n",
    "\n",
    "    mtg_train_dataset = MTG_Mel_ContrastiveDataset(mtg_mel_path, references_sample_rate, mask_prob=0.8, samples_per_file=15, folder_whitelist=train_folders, concurrent_files=batch_size)\n",
    "    mtg_val_dataset = MTG_Mel_ContrastiveDataset(mtg_mel_path, references_sample_rate, mask_prob=0.8, samples_per_file=15, folder_whitelist=val_folders, max_files=2*batch_size, concurrent_files=batch_size)\n",
    "\n",
    "    train_loader = DataLoader(mtg_train_dataset, batch_size=batch_size, shuffle=False, num_workers=2, prefetch_factor=2)\n",
    "    val_loader = DataLoader(mtg_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    contrastive_embedder_model = model\n",
    "    contrastive_embedder_model = contrastive_embedder_model.to(device)\n",
    "\n",
    "\n",
    "    USE_LATEST_CHECKPOINT = True\n",
    "    checkpoint_file = output_dir / f'{get_model_name(contrastive_embedder_model.name, learning_rate, batch_size)}_latest_checkpoint.pth' if USE_LATEST_CHECKPOINT else None\n",
    "    if checkpoint_file is not None and not checkpoint_file.exists():\n",
    "        print(f\"Checkpoint file {checkpoint_file} not found, starting with a fresh model\")\n",
    "        checkpoint_file = None\n",
    "    else:\n",
    "        print(f\"Recovering model from {checkpoint_file}\")\n",
    "\n",
    "\n",
    "    t_loss, v_loss = [], []\n",
    "    _ = train_contrastive_model(\n",
    "        contrastive_embedder_model,\n",
    "        train_loader, val_loader, epochs=epochs, lr=learning_rate, batch_size=batch_size, criterion = criterion,\n",
    "        t_loss_history=t_loss, v_loss_history=v_loss,\n",
    "        output_dir=output_dir,\n",
    "        checkpoint_file=checkpoint_file,\n",
    "        device = device\n",
    "    )\n",
    "    \n",
    "    losses = {'train_loss': t_loss, 'val_loss': v_loss}\n",
    "    with open(output_dir / f'{get_model_name(contrastive_embedder_model.name, learning_rate, batch_size)}.pkl', 'wb') as file:\n",
    "        pickle.dump(losses, file)\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(t_loss, v_loss):\n",
    "    # Plot the losses\n",
    "    contrastive_loss_fig, contrastive_loss_axs = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    contrastive_loss_axs.plot(*zip(*t_loss), label='Training Loss')\n",
    "    contrastive_loss_axs.plot(*zip(*v_loss), label='Validation Loss')\n",
    "    # contrastive_loss_axs.plot(*zip(*t_loss), label='Training Loss')\n",
    "    contrastive_loss_axs.legend()\n",
    "    contrastive_loss_axs.set_xlabel('Step')\n",
    "    contrastive_loss_axs.set_ylabel('InfoNCE Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint file output/contrastive/GTZANContrastiveModelLarge_CONT_DIM(128)_(3)_LR(0.001)_BS(512)_latest_checkpoint.pth not found, starting with a fresh model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]ERROR:tornado.general:SEND Error: Host unreachable\n"
     ]
    }
   ],
   "source": [
    "model = GTZANContrastiveModelLarge(128)\n",
    "criterion = InfoNCE()\n",
    "output_dir = contrastive_output_dir\n",
    "batch_size = 512\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "train_model_with_params(batch_size=batch_size, learning_rate=learning_rate, epochs = epochs, model=model, output_dir = output_dir, criterion = criterion )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
